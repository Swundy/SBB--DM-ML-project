!pip install evaluate
!pip install datasets
!pip install sentencepiece
!pip install transformers

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

import pandas as pd
import os
import numpy as np
import torch
import torch.nn.functional as F
from datasets import load_dataset
from sklearn.model_selection import train_test_split
from transformers import CamembertTokenizer, CamembertForSequenceClassification

path = "drive/MyDrive"
data = pd.read_csv(os.path.join(path, "training_data.csv"))
data["labels"] = pd.Categorical(data["difficulty"]).codes
# data["labels"] = data["labels"].apply(lambda x: F.one_hot(x=torch.tensor(x), num_classes=6))

train, test = train_test_split(data, test_size=0.2, stratify=data["labels"].to_list(), random_state=42)

!mkdir "drive/MyDrive/training"

train.to_csv(os.path.join(path, "training", "train.csv"), index=False)
test.to_csv(os.path.join(path, "training", "test.csv"), index=False)

data = load_dataset(os.path.join(path, "training"))
data = data.remove_columns(["id", "difficulty"])
id2label = {0: "A1", 1: "A2", 2: "B1", 3: "B2", 4: "C1", 5: "C2"}
label2id = {"A1": 0, "A2": 1, "B1": 2, "B2": 3, "C1": 4, "C2": 5}
tokenizer = CamembertTokenizer.from_pretrained("camembert-base")
model = CamembertForSequenceClassification.from_pretrained("camembert-base", num_labels=6, id2label=id2label, label2id=label2id)

import evaluate

accuracy = evaluate.load("accuracy")

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    return accuracy.compute(predictions=predictions, references=labels)
    
def preprocess_function(examples):
    return tokenizer(examples["sentence"], truncation=True, max_length=512, padding="max_length", return_tensors="pt")

tokenized_data = data.map(preprocess_function, batched=True)
tokenized_data.set_format("pt", columns = ["input_ids", "attention_mask"], output_all_columns=True)
tokenized_data = tokenized_data.remove_columns(["sentence"])

from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
torch.cuda.empty_cache()
from transformers import TrainingArguments, Trainer

training_args = TrainingArguments(
    output_dir="trained_model",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=5,
    weight_decay=0.001,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    push_to_hub=False,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_data["train"],
    eval_dataset=tokenized_data["test"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

trainer.train()

i = 91

print(data["test"]["sentence"][i], data["test"]["labels"][i])

device = torch.device('cuda:0')
input1 = torch.reshape(tokenized_data["test"]["input_ids"][i], (1, -1)).to(device)
input2 = torch.reshape(tokenized_data["test"]["attention_mask"][i], (1, -1)).to(device)

model(input_ids= input1,
      attention_mask = input2)

